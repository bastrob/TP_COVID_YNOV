{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projet de groupe 1\n",
    "\n",
    "BOUSQUIE Johan (préparation et la structuration des données)\n",
    "\n",
<<<<<<< HEAD
    "ROBERT Bastien (scripts: recupération des données, fonction nb, preparation tweets, map interactive )\n",
=======
    "ROBERT Bastien (récolte des données)\n",
>>>>>>> d37676159eeb8baecaacabd6ec033a50b7e47519
    "\n",
    "ROBERT Simon (Analyse et Visualisation des données) \n",
    "\n",
    "SORBY Franck (Analyse et visualisation des données)\n",
    "\n",
    "\n",
    "# Projet de groupe 1 : Récolte de données sur la pandémie du Covid-19\n",
    "\n",
    "\n",
    "## Introduction\n",
    "\n",
    "L'objectif de ce projet était de récolter, structurer et analyser des données en relation avec la pandémie de Covid 19. \n",
    "\n",
    "Avec cet objectif, nous avons donc décidé d'orienter notre analyse autour des problématiques suivantes: \n",
    "\n",
    "* Le facteur humain a-t-il un impact sur l’évolution du covid ? \n",
    "* L’opinion et le comportement des gens ont-ils un impact sur l’évolution du COVID ? \n",
    "* Existe-t-il un lien entre le ressenti des gens et l’évolution de la pandémie ? \n",
    "\n",
    "## Partie 1: Récolte des données\n",
    "\n",
    "\n",
    "Afin de répondre à nos problématiques, un ensemble de données doivent être récoltées. \n",
    "\n",
    "Dans un premier temps, nous allons récolter l'ensemble des données relatives à l'évolution de l'épidémie en France, départements par départements. \n",
    "\n",
    "Ensuite il nous fallait récolter un ensemble de réactions afin d'évaluer l'impact moral de la pandémie.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scrapping évolution Covid 19\n",
<<<<<<< HEAD
    "Récupération des données libres (voir licence dans script coronaviusAPI_france) du gouvernement par le biais d'une API. Les données étaient accessibles par département depuis le début du premier confinement au format json. (Il fallait aussi relancer régulierement le script car les données étaient mises à jour tous les jours à 8 heures).\n",
    "Il a donc fallu créer un csv pour chaque département avec les informations que nous avions ciblés.\n",
    "\n",
    "\n",
    "### Preparation évolution Covid 19\n",
    "Dans le script de préparation nous avons commencé par aggréger l'ensemble des départements dans un seul et même fichier csv.\n",
    "Nous avons ensuite ajouté plusieurs colonnes comme la région, la population, les coordonnées lat long parceque nous avons fait le choix de visualiser l'évolution du Covid par région.\n",
    "Toutes ces informations n'étant pas disponibles de base dans le fichier brut, nous avons dû effectuer des recherches pour les récupérer.\n",
    "Pour récupérer ces coordonnées, nous avons utilisé l'API Nominatim d'OpenStreetMap. Pour certaines visualisations (voir fn.create_map_france_2), nous avons récuperé des fichiers au format shapefile et avons manipulé des GeoDataFrame.\n",
    "Lorsque le processus de préparation est terminé, le fichier csv est sauvegardé dans le dossier /datasets_cleaned.\n",
    "\n",
    "### Scrapping Twitter\n",
    "L'objectif était de récupérer des tweets provenant des deux confinements, pour comprendre l'état d'esprits des français et leur ressenti par rapport à ces deux périodes.\n",
    "Pour récupérer des tweets du premier confinement nous avons dû récupérer les tweets à partir des ID, (https://github.com/calciu/COVID19-LockdownFr), et passer par l'API de Twitter en utlisant tweepy. La constitution des datasets fut longue car Twitter limite le nombre de requêtes.\n",
    "\n",
    "Pour le second confinement, nous avons utilisé l'API Stream de Twitter qui permettait de récupérer en temps réel les tweets postés. Nous avons configurés une liste d'hashtags pour sélectionner uniquement ceux qui nous intéressaient.\n",
    "\n",
    "### Preparation Twitter\n",
    "Nous avons appliqué plusieurs traitements, nettoyage du tweet, utilisation de regex, extraction des émojis et des hashtags, supression des stop_words et caractères spéciaux. De plus, nous avons ajouté une colonne qui montre la popularité d'un tweet, en comptant le nombre de fois que ce tweet a été retweeté.\n",
    "\n",
    "Les licences sont renseignées dans les scripts où dans les dossiers contenant les données.\n",
    "Nous avons recueillis 170k de tweets du premiers confinement (environ 30k en enlevant les RT) et 9k du second confinement.\n",
    "\n",
    "Les fichiers raw json sont stockés en local sur un DD, Github limite le stockage des fichiers volumineux (format LFS)."
=======
    "Récupération des données libres (voir licence dans script coronaviusAPI_france) du gouvernement par le biais d'une API. Les données étaient accessibles par département depuis le début du premier confinement\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Scrapping Twitter\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "A compléter"
>>>>>>> d37676159eeb8baecaacabd6ec033a50b7e47519
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partie 2: Analyse des données\n",
    "\n",
    "\n",
    "A compléter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
